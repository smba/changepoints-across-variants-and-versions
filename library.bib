
@inproceedings{abal_42_2014,
  title = {42 Variability Bugs in the Linux Kernel: A Qualitative Analysis},
  shorttitle = {42 Variability Bugs in the Linux Kernel},
  booktitle = {Proceedings of the 29th {{ACM}}/{{IEEE}} International Conference on {{Automated}} Software Engineering},
  author = {Abal, Iago and Brabrand, Claus and Wasowski, Andrzej},
  year = {2014},
  month = sep,
  pages = {421--432},
  publisher = {{Association for Computing Machinery}},
  address = {{Vasteras, Sweden}},
  doi = {10.1145/2642937.2642990},
  abstract = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
  file = {/home/stefan/Zotero/storage/DBTVTQK7/Abal et al. - 2014 - 42 variability bugs in the linux kernel a qualita.pdf},
  isbn = {978-1-4503-3013-8},
  keywords = {bugs,feature interactions,linux,software variability},
  series = {{{ASE}} '14}
}

@article{abal_variability_2018,
  title = {Variability {{Bugs}} in {{Highly Configurable Systems}}: {{A Qualitative Analysis}}},
  shorttitle = {Variability {{Bugs}} in {{Highly Configurable Systems}}},
  author = {Abal, Iago and Melo, Jean and St{\u a}nciulescu, {\c S}tefan and Brabrand, Claus and Ribeiro, M{\'a}rcio and W{\k{a}}sowski, Andrzej},
  year = {2018},
  month = jan,
  volume = {26},
  pages = {10:1--10:34},
  issn = {1049-331X},
  doi = {10.1145/3149119},
  abstract = {Variability-sensitive verification pursues effective analysis of the exponentially many variants of a program family. Several variability-aware techniques have been proposed, but researchers still lack examples of concrete bugs induced by variability, occurring in real large-scale systems. A collection of real world bugs is needed to evaluate tool implementations of variability-sensitive analyses by testing them on real bugs. We present a qualitative study of 98 diverse variability bugs (i.e., bugs that occur in some variants and not in others) collected from bug-fixing commits in the Linux, Apache, BusyBox, and Marlin repositories. We analyze each of the bugs, and record the results in a database. For each bug, we create a self-contained simplified version and a simplified patch, in order to help researchers who are not experts on these subject studies to understand them, so that they can use these bugs for evaluation of their tools. In addition, we provide single-function versions of the bugs, which are useful for evaluating intra-procedural analyses. A web-based user interface for the database allows to conveniently browse and visualize the collection of bugs. Our study provides insights into the nature and occurrence of variability bugs in four highly-configurable systems implemented in C/C++, and shows in what ways variability hinders comprehension and the uncovering of software bugs.},
  file = {/home/stefan/Zotero/storage/6ZYB4Y6J/Abal et al. - 2018 - Variability Bugs in Highly Configurable Systems A.pdf},
  journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
  keywords = {Bugs,feature interactions,Linux,software variability},
  number = {3}
}

@article{alcocer_horizontal_nodate,
  title = {{{HORIZONTAL PROFILING}}: {{A SAMPLING TECHNIQUE TO IDENTIFY PERFORMANCE REGRESSIONS}}},
  author = {Alcocer, Juan Pablo Sandoval},
  pages = {110},
  file = {/home/stefan/Zotero/storage/383VBCYC/Alcocer - HORIZONTAL PROFILING A SAMPLING TECHNIQUE TO IDEN.pdf},
  language = {en}
}



@article{amershi_software_2019,
  title = {Software {{Engineering}} for {{Machine Learning}}: {{A Case Study}}},
  shorttitle = {Software {{Engineering}} for {{Machine Learning}}},
  author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald C. and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  year = {2019},
  pages = {291--300},
  doi = {10.1109/ICSE-SEIP.2019.00042},
  abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be "entangled" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
  file = {/home/stefan/Zotero/storage/R5TB23XW/measureDeeper.pdf},
  journal = {2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  keywords = {Agile software development,Application domain,Applications of artificial intelligence,Best practice,Capability Maturity Model,Component-based software engineering,Data science,Machine learning,Natural language processing,Software deployment}
}

@inproceedings{avritzer_ensuring_2005,
  title = {Ensuring Stable Performance for Systems That Degrade},
  booktitle = {Proceedings of the 5th International Workshop on {{Software}} and Performance  - {{WOSP}} '05},
  author = {Avritzer, Alberto and Bondi, Andre and Weyuker, Elaine J.},
  year = {2005},
  pages = {43--51},
  publisher = {{ACM Press}},
  address = {{Palma, Illes Balears, Spain}},
  doi = {10.1145/1071021.1071026},
  abstract = {A new approach that is useful in identifying and eliminating performance degradation occurring in aging software is proposed. A customer-affecting metric is used to initiate the restoration of such a system to full capacity. A case study is described in which, by simulating an industrial software system, we are able to show that by monitoring a customeraffecting metric and frequently comparing its degradation to the performance objective, we can ensure system stability at a very low cost.},
  file = {/home/stefan/Zotero/storage/SDK979MG/Avritzer et al. - 2005 - Ensuring stable performance for systems that degra.pdf},
  isbn = {978-1-59593-087-3},
  language = {en}
}

@inproceedings{batory_feature_2011,
  title = {Feature Interactions, Products, and Composition},
  booktitle = {Proceedings of the 10th {{ACM}} International Conference on {{Generative}} Programming and Component Engineering - {{GPCE}} '11},
  author = {Batory, Don and H{\"o}fner, Peter and Kim, Jongwook},
  year = {2011},
  pages = {13},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/2047862.2047867},
  abstract = {The relationship between feature modules and feature interactions is not well-understood. To explain classic examples of feature interaction, we show that features are not only composed sequentially, but also by cross-product and interaction operations that heretofore were implicit in the literature. Using the CIDE tool as our starting point, we (a) present a formal model of these operations, (b) show how it connects and explains previously unrelated results in Feature Oriented Software Development (FOSD), and (c) describe a tool, based on our formalism, that demonstrates how changes in composed documents can be back-propagated to their original feature module definitions, thereby improving FOSD tooling.},
  file = {/home/stefan/Zotero/storage/S6M29JTW/Batory et al. - 2011 - Feature interactions, products, and composition.pdf},
  isbn = {978-1-4503-0689-8},
  language = {en}
}

@article{bulej_repeated_2005,
  title = {Repeated Results Analysis for Middleware Regression Benchmarking},
  author = {Bulej, Lubom{\'i}r and Kalibera, Tom{\'a}{\v s} and T{\r{u}}ma, Petr},
  year = {2005},
  month = may,
  volume = {60},
  pages = {345--358},
  issn = {01665316},
  doi = {10.1016/j.peva.2004.10.013},
  abstract = {The paper outlines the concept of regression benchmarking as a variant of regression testing focused at detecting performance regressions. Applying the regression benchmarking in the area of middleware development, the paper explains how the regression benchmarking differs from middleware benchmarking in general, and shows on realworld examples why the existing benchmarks do not give results sufficient for regression benchmarking. Considering two broad groups of benchmarks based on their complexity, novel techniques are proposed for the repeated analysis of results for the purpose of detecting performance regressions.},
  file = {/home/stefan/Zotero/storage/I69ZVGM6/Bulej et al. - 2005 - Repeated results analysis for middleware regressio.pdf},
  journal = {Performance Evaluation},
  language = {en},
  number = {1-4}
}

@inproceedings{cashman_dna_2019,
  title = {{{DNA}} as {{Features}}: {{Organic Software Product Lines}}},
  shorttitle = {{{DNA}} as {{Features}}},
  booktitle = {Proceedings of the 23rd {{International Systems}} and {{Software Product Line Conference}} - {{Volume A}}},
  author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
  year = {2019},
  month = sep,
  pages = {108--118},
  publisher = {{Association for Computing Machinery}},
  address = {{Paris, France}},
  doi = {10.1145/3336294.3336298},
  abstract = {Software product line engineering is a best practice for managing reuse in families of software systems. In this work, we explore the use of product line engineering in the emerging programming domain of synthetic biology. In synthetic biology, living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse these programs. As a first step towards this goal, we perform a domain engineering case study that leverages an open-source repository of more than 45,000 reusable DNA parts. We are able to identify features and their related artifacts, all of which can be composed to make different programs. We demonstrate that we can successfully build feature models representing families for two commonly engineered functions. We then analyze an existing synthetic biology case study and demonstrate how product line engineering can be beneficial in this domain.},
  file = {/home/stefan/Zotero/storage/4TSZIG88/Cashman et al. - 2019 - DNA as Features Organic Software Product Lines.pdf},
  isbn = {978-1-4503-7138-4},
  keywords = {BioBricks,software product lines,synthetic biology},
  series = {{{SPLC}} '19}
}

@incollection{casteleyn_identifying_2014,
  title = {Identifying {{Root Causes}} of {{Web Performance Degradation Using Changepoint Analysis}}},
  booktitle = {Web {{Engineering}}},
  author = {Cito, J{\"u}rgen and Suljoti, Dritan and Leitner, Philipp and Dustdar, Schahram},
  editor = {Casteleyn, Sven and Rossi, Gustavo and Winckler, Marco},
  year = {2014},
  volume = {8541},
  pages = {181--199},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08245-5_11},
  abstract = {The large scale of the Internet has offered unique economic opportunities, that in turn introduce overwhelming challenges for development and operations to provide reliable and fast services in order to meet the high demands on the performance of online services. In this paper, we investigate how performance engineers can identify three different classes of externally-visible performance problems (global delays, partial delays, periodic delays) from concrete traces. We develop a simulation model based on a taxonomy of root causes in server performance degradation. Within an experimental setup, we obtain results through synthetic monitoring of a target Web service, and observe changes in Web performance over time through exploratory visual analysis and changepoint detection. Finally, we interpret our findings and discuss various challenges and pitfalls.},
  file = {/home/stefan/Zotero/storage/U4KRN7X8/Cito et al. - 2014 - Identifying Root Causes of Web Performance Degrada.pdf},
  isbn = {978-3-319-08244-8 978-3-319-08245-5},
  language = {en}
}

@inproceedings{chen_generating_2016,
  title = {Generating Performance Distributions via Probabilistic Symbolic Execution},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Chen, Bihuan and Liu, Yang and Le, Wei},
  year = {2016},
  month = may,
  pages = {49--60},
  publisher = {{Association for Computing Machinery}},
  address = {{Austin, Texas}},
  doi = {10.1145/2884781.2884794},
  abstract = {Analyzing performance and understanding the potential best-case, worst-case and distribution of program execution times are very important software engineering tasks. There have been model-based and program analysis-based approaches for performance analysis. Model-based approaches rely on analytical or design models derived from mathematical theories or software architecture abstraction, which are typically coarse-grained and could be imprecise. Program analysis-based approaches collect program profiles to identify performance bottlenecks, which often fail to capture the overall program performance. In this paper, we propose a performance analysis framework PerfPlotter. It takes the program source code and usage profile as inputs and generates a performance distribution that captures the input probability distribution over execution times for the program. It heuristically explores high-probability and low-probability paths through probabilistic symbolic execution. Once a path is explored, it generates and runs a set of test inputs to model the performance of the path. Finally, it constructs the performance distribution for the program. We have implemented PerfPlotter based on the Symbolic PathFinder infrastructure, and experimentally demonstrated that PerfPlotter could accurately capture the best-case, worst-case and distribution of program execution times. We also show that performance distributions can be applied to various important tasks such as performance understanding, bug validation, and algorithm selection.},
  file = {/home/stefan/Zotero/storage/35XGQ9RH/Chen et al. - 2016 - Generating performance distributions via probabili.pdf},
  isbn = {978-1-4503-3900-1},
  keywords = {performance analysis,symbolic execution},
  note = {http://web.archive.org/web/20200214075728/https://dl.acm.org/doi/10.1145/2884781.2884794},
  series = {{{ICSE}} '16}
}

@inproceedings{daly_industry_2020,
  title = {Industry {{Paper}}: {{The Use}} of {{Change Point Detection}} to {{Identify Software Performance Regressions}} in a {{Continuous Integration System}}},
  booktitle = {11th {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Daly, David and Brown, William and Ingo, Henrik and O'Leary, Jim and Bradford, David},
  year = {2020},
  pages = {9},
  publisher = {{ACM}},
  address = {{Edmonton, Canada}},
  abstract = {We describe our process for automatic detection of performance changes for a software product in the presence of noise. A large collection of tests run periodically as changes to our software product are committed to our source repository, and we would like to identify the commits responsible for performance regressions. Previously, we relied on manual inspection of time series graphs to identify significant changes. That was later replaced with a threshold-based detection system, but neither system was sufficient for finding changes in performance in a timely manner. This work describes our recent implementation of a change point detection system built upon the E-Divisive means [14] algorithm. The algorithm produces a list of change points representing significant changes from a given history of performance results. A human reviews the list of change points for actionable changes, which are then triaged for further inspection. Using change point detection has had a dramatic impact on our ability to detect performance changes. Quantitatively, it has dramatically dropped our false positive rate for performance changes, while qualitatively it has made the entire performance evaluation process easier, more productive (ex. catching smaller regressions), and more timely.},
  file = {/home/stefan/Zotero/storage/GIKISS2F/Daly et al. - 2020 - Industry Paper The Use of Change Point Detection .pdf},
  language = {en}
}

@inproceedings{dorn_generating_2020,
  title = {Generating Attributed Variability Models for Transfer Learning},
  booktitle = {Proceedings of the 14th {{International Working Conference}} on {{Variability Modelling}} of {{Software}}-{{Intensive Systems}}},
  author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
  year = {2020},
  month = feb,
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{Magdeburg, Germany}},
  doi = {10.1145/3377024.3377040},
  abstract = {Modern software systems often provide configuration options for customizing of the system's functional and non-functional properties, such as response time and energy consumption. The valid configurations of a software system are commonly documented in a variability model. Supporting the optimization of a system's non-functional properties, variability models have been extended with attributes that represent the influence of one or multiple options on a property. The concrete values of attributes are typically determined only in a single environment (e.g., for a specific software version, a certain workload, and a specific hardware setup) and are applicable only for this context. Changing the environment, attribute values need to be updated. Instead of determining all attributes from scratch with new measurements, recent approaches rely on transfer learning to reduce the effort of obtaining new attribute values. However, the development and evaluation of new transfer-learning techniques requires extensive measurements by themselves, which often is prohibitively costly. To support research in this area, we propose an approach to synthesize realistic attributed variability models from a base model. This way, we can support research and validation of novel transfer-learning techniques for configurable software systems. We use a genetic algorithm to vary attribute values. Combined with a declarative objective function, we search a changed attributed variability model that keeps some key characteristics while mimicking realistic changes of individual attribute values. We demonstrate the applicability of our approach by replicating the evaluation of an existing transfer-learning technique.},
  file = {/home/stefan/Zotero/storage/YJI6Y6JL/Dorn et al. - 2020 - Generating attributed variability models for trans.pdf},
  isbn = {978-1-4503-7501-6},
  keywords = {attributed variability models,Loki,transfer learning,variability modelling},
  series = {{{VAMOS}} '20}
}

@inproceedings{duarte_learning_2018,
  title = {Learning Non-Deterministic Impact Models for Adaptation},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Software Engineering}} for {{Adaptive}} and {{Self}}-{{Managing Systems}}  - {{SEAMS}} '18},
  author = {Duarte, Francisco and Gil, Richard and Romano, Paolo and Lopes, Ant{\'o}nia and Rodrigues, Lu{\'i}s},
  year = {2018},
  pages = {196--205},
  publisher = {{ACM Press}},
  address = {{Gothenburg, Sweden}},
  doi = {10.1145/3194133.3194138},
  abstract = {Many adaptive systems react to variations in their environment by changing their con\dbend{}guration. Often, they make the adaptation decisions based on some knowledge about how the recon\dbend{}guration actions impact the key performance indicators. However, the outcome of these actions is typically a\dbend{}ected by uncertainty. Adaptation actions have non-deterministic impacts, potentially leading to multiple outcomes. When this uncertainty is not captured explicitly in the models that guide adaptation, decisions may turn out ine\dbend{}ective or even harmful to the system. Also critical is the need for these models to be interpretable to the human operators that are accountable for the system. However, accurate impact models for actions that result in non-deterministic outcomes are very di\dbend{}cult to obtain and existing techniques that support the automatic generation of these models, mainly based on machine learning, are limited in the way they learn non-determinism.},
  file = {/home/stefan/Zotero/storage/PPAKQSXZ/Duarte et al. - 2018 - Learning non-deterministic impact models for adapt.pdf},
  isbn = {978-1-4503-5715-9},
  language = {en}
}

@inproceedings{fischer_stack_2019,
  title = {Stack Overflow Considered Helpful! Deep Learning Security Nudges towards Stronger Cryptography},
  booktitle = {28th \$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ {{Security Symposium}} (\$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ {{Security}} 19)},
  author = {Fischer, Felix and Xiao, Huang and Kao, Ching-Yu and Stachelscheid, Yannick and Johnson, Benjamin and Razar, Danial and Fawkesley, Paul and Buckley, Nat and B{\"o}ttinger, Konstantin and Muntean, Paul and others},
  year = {2019},
  pages = {339--356},
  file = {/home/stefan/Zotero/storage/6WZWVQV9/Fischer et al. - Stack Overﬂow Considered Helpful! Deep Learning Se.pdf}
}

@article{fischer_stack_nodate,
  title = {Stack {{Overflow Considered Helpful}}! {{Deep Learning Security Nudges Towards Stronger Cryptography}}},
  author = {Fischer, Felix and Xiao, Huang and Kao, Ching-Yu and Stachelscheid, Yannick and Johnson, Benjamin and Razar, Danial and Fawkesley, Paul and Buckley, Nat and Bottinger, Konstantin and Muntean, Paul and Grossklags, Jens},
  pages = {19},
  abstract = {Stack Overflow is the most popular discussion platform for software developers. However, recent research identified a large amount of insecure encryption code in production systems that has been inspired by examples given on Stack Overflow. By copying and pasting functional code, developers introduced exploitable software vulnerabilities into security-sensitive high-profile applications installed by millions of users every day.},
  language = {en}
}

@article{fischer_stack_nodate-1,
  title = {Stack {{Overflow Considered Helpful}}! {{Deep Learning Security Nudges Towards Stronger Cryptography}}},
  author = {Fischer, Felix and Xiao, Huang and Kao, Ching-Yu and Stachelscheid, Yannick and Johnson, Benjamin and Razar, Danial and Fawkesley, Paul and Buckley, Nat and Bottinger, Konstantin and Muntean, Paul and Grossklags, Jens},
  pages = {18},
  abstract = {Stack Overflow is the most popular discussion platform for software developers. However, recent research identified a large amount of insecure encryption code in production systems that has been inspired by examples given on Stack Overflow. By copying and pasting functional code, developers introduced exploitable software vulnerabilities into security-sensitive high-profile applications installed by millions of users every day.},
  file = {/home/stefan/Zotero/storage/UJWZYI3X/Fischer et al. - Stack Overﬂow Considered Helpful! Deep Learning Se.pdf},
  language = {en}
}

@inproceedings{ghaith_profile-based_2013,
  title = {Profile-{{Based}}, {{Load}}-{{Independent Anomaly Detection}} and {{Analysis}} in {{Performance Regression Testing}} of {{Software Systems}}},
  booktitle = {Proceedings of the {{Euromicro Conference}} on {{Software Maintenance}} and {{Reengineering}}, {{CSMR}}},
  author = {Ghaith, Shadi and Wang, Miao and Perry, P. and Murphy, John},
  year = {2013},
  month = mar,
  pages = {379--383},
  doi = {10.1109/CSMR.2013.54},
  abstract = {Performance evaluation through regression testing is an important step in the software production process. It aims to make sure that the performance of new releases do not regress under a field-like load. The main outputs of regression tests are the metrics that represent the response time of various transactions as well as the resource utilization (CPU, disk I/Oand Network). In this paper, we propose to use a concept known as Transaction Profile, which can provide a detailed representation for the transaction in a load independent manner, to detect anomalies through performance test runs. The approach uses data readily available in performance regression tests and a queueing network model of the system under test to infer the Transactions Profiles. Our initial results show that the Transactions Profiles calculated from load regression test data uncover the performance impact of any update to the software. Therefore we conclude that using Transactions Profiles is an effective approach to allow testing teams to easily assure each new software release does not suffer performance regression.},
  file = {/home/stefan/Zotero/storage/E6CER6GV/Ghaith et al. - 2013 - Profile-Based, Load-Independent Anomaly Detection .pdf},
  isbn = {978-1-4673-5833-0}
}

@article{guo_genetic_2011,
  title = {A Genetic Algorithm for Optimized Feature Selection with Resource Constraints in Software Product Lines},
  author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
  year = {2011},
  month = dec,
  volume = {84},
  pages = {2208--2221},
  issn = {01641212},
  doi = {10.1016/j.jss.2011.06.026},
  abstract = {Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents GAFES, an artificial intelligence approach, based on genetic algorithms (GAs), for optimized feature selection in SPLs. Our empirical results show that GAFES can produce solutions with 86-97\% of the optimality of other automated feature selection algorithms and in 45-99\% less time than existing exact and heuristic feature selection techniques.},
  file = {/home/stefan/Zotero/storage/865DTWYT/Guo et al. - 2011 - A genetic algorithm for optimized feature selectio.pdf},
  journal = {Journal of Systems and Software},
  language = {en},
  number = {12}
}

@inproceedings{guo_variability-aware_2013,
  title = {Variability-Aware Performance Prediction: {{A}} Statistical Learning Approach},
  shorttitle = {Variability-Aware Performance Prediction},
  booktitle = {2013 28th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Guo, Jianmei and Czarnecki, Krzysztof and Apel, Sven and Siegmund, Norbert and Wasowski, Andrzej},
  year = {2013},
  month = nov,
  pages = {301--311},
  publisher = {{IEEE}},
  address = {{Silicon Valley, CA, USA}},
  doi = {10.1109/ASE.2013.6693089},
  abstract = {Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94 \% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing technique and guide users to choose one or the other in practice.},
  file = {/home/stefan/Zotero/storage/JFWCLWEI/Guo et al. - 2013 - Variability-aware performance prediction A statis.pdf},
  isbn = {978-1-4799-0215-6},
  language = {en}
}

@inproceedings{ha_deepperf_2019,
  title = {{{DeepPerf}}: {{Performance Prediction}} for {{Configurable Software}} with {{Deep Sparse Neural Network}}},
  shorttitle = {{{DeepPerf}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Ha, Huong and Zhang, Hongyu},
  year = {2019},
  month = may,
  pages = {1095--1106},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICSE.2019.00113},
  abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
  file = {/home/stefan/Zotero/storage/U5M8E8FL/Ha and Zhang - 2019 - DeepPerf Performance Prediction for Configurable .pdf},
  isbn = {978-1-72810-869-8},
  language = {en}
}

@inproceedings{han_empirical_2016,
  title = {An {{Empirical Study}} on {{Performance Bugs}} for {{Highly Configurable Software Systems}}},
  booktitle = {Proceedings of the 10th {{ACM}}/{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Han, Xue and Yu, Tingting},
  year = {2016},
  month = sep,
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{Ciudad Real, Spain}},
  doi = {10.1145/2961111.2962602},
  abstract = {Modern computer systems are highly-configurable, complicating the testing and debugging process. The sheer size of the configuration space makes the quality of software even harder to achieve. Performance is one of the key aspects of non-functional qualities, where performance bugs can cause significant performance degradation and lead to poor user experience. However, performance bugs are difficult to expose, primarily because detecting them requires specific inputs, as well as a specific execution environment (e.g., configurations). While researchers have developed techniques to analyze, quantify, detect, and fix performance bugs, we conjecture that many of these techniques may not be effective in highly-configurable systems. In this paper, we study the challenges that configurability creates for handling performance bugs. We study 113 real-world performance bugs, randomly sampled from three highly-configurable open-source projects: Apache, MySQL and Firefox. The findings of this study provide a set of lessons learned and guidance to aid practitioners and researchers to better handle performance bugs in highly-configurable software systems.},
  file = {/home/stefan/Zotero/storage/DAM89MH9/Han und Yu - 2016 - An Empirical Study on Performance Bugs for Highly .pdf},
  isbn = {978-1-4503-4427-2},
  keywords = {Configuration,Empirical Study,Performance},
  series = {{{ESEM}} '16}
}

@inproceedings{harman_current_2007,
  title = {The {{Current State}} and {{Future}} of {{Search Based Software Engineering}}},
  booktitle = {Future of {{Software Engineering}} ({{FOSE}} '07)},
  author = {Harman, Mark},
  year = {2007},
  month = may,
  pages = {342--357},
  publisher = {{IEEE}},
  address = {{Minneapolis, MN, USA}},
  doi = {10.1109/FOSE.2007.29},
  abstract = {This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.},
  file = {/home/stefan/Zotero/storage/Q2E7XPCU/Harman - 2007 - The Current State and Future of Search Based Softw.pdf},
  isbn = {978-0-7695-2829-8},
  language = {en}
}

@inproceedings{henning_scalable_2019,
  title = {A {{Scalable Architecture}} for {{Power Consumption Monitoring}} in {{Industrial Production Environments}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Fog Computing}} ({{ICFC}})},
  author = {Henning, S{\"o}ren and Hasselbring, Wilhelm and M{\"o}bius, Armin},
  year = {2019},
  month = jun,
  pages = {124--133},
  doi = {10.1109/ICFC.2019.00024},
  abstract = {Detailed knowledge about the electrical power consumption in industrial production environments is a prerequisite to reduce and optimize their power consumption. Today's industrial production sites are equipped with a variety of sensors that, inter alia, monitor electrical power consumption in detail. However, these environments often lack an automated data collation and analysis. We present a system architecture that integrates different sensors and analyzes and visualizes the power consumption of devices, machines, and production plants. It is designed with a focus on scalability to support production environments of various sizes and to handle varying loads. We argue that a scalable architecture in this context must meet requirements for fault tolerance, extensibility, real-time data processing, and resource efficiency. As a solution, we propose a microservice-based architecture augmented by big data and stream processing techniques. Applying the fog computing paradigm, parts of it are deployed in an elastic, central cloud while other parts run directly, decentralized in the production environment. A prototype implementation of this architecture presents solutions how different kinds of sensors can be integrated and their measurements can be continuously aggregated. In order to make analyzed data comprehensible, it features a single-page web application that provides different forms of data visualization. We deploy this pilot implementation in the data center of a medium-sized enterprise, where we successfully monitor the power consumption of 16\textasciitilde{}servers. Furthermore, we show the scalability of our architecture with 20,000\textasciitilde{}simulated sensors.},
  archivePrefix = {arXiv},
  eprint = {1907.01046},
  eprinttype = {arxiv},
  file = {/home/stefan/Zotero/storage/WTKDM97K/Henning et al. - 2019 - A Scalable Architecture for Power Consumption Moni.pdf;/home/stefan/Zotero/storage/NMRNTU7U/1907.html},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Software Engineering}
}

@incollection{hutchison_statistical_2010,
  title = {Statistical {{Inference}} of {{Software Performance Models}} for {{Parametric Performance Completions}}},
  booktitle = {Research into {{Practice}} \textendash{} {{Reality}} and {{Gaps}}},
  author = {Happe, Jens and Westermann, Dennis and Sachs, Kai and Kapov{\'a}, Lucia},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Heineman, George T. and Kofron, Jan and Plasil, Frantisek},
  year = {2010},
  volume = {6093},
  pages = {20--35},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-13821-8_4},
  abstract = {Software performance engineering (SPE) enables software architects to ensure high performance standards for their applications. However, applying SPE in practice is still challenging. Most enterprise applications include a large software basis, such as middleware and legacy systems. In many cases, the software basis is the determining factor of the system's overall timing behavior, throughput, and resource utilization. To capture these influences on the overall system's performance, established performance prediction methods (modelbased and analytical) rely on models that describe the performance-relevant aspects of the system under study. Creating such models requires detailed knowledge on the system's structure and behavior that, in most cases, is not available. In this paper, we abstract from the internal structure of the system under study. We focus on message-oriented middleware (MOM) and analyze the dependency between the MOM's usage and its performance. We use statistical inference to conclude these dependencies from observations. For ActiveMQ 5.3, the resulting functions predict the performance with a relative mean square error 0.1.},
  file = {/home/stefan/Zotero/storage/PEZH5PQN/Happe et al. - 2010 - Statistical Inference of Software Performance Mode.pdf},
  isbn = {978-3-642-13820-1 978-3-642-13821-8},
  language = {en}
}

@incollection{hutchison_statistical_2010-1,
  title = {Statistical {{Inference}} of {{Software Performance Models}} for {{Parametric Performance Completions}}},
  booktitle = {Research into {{Practice}} \textendash{} {{Reality}} and {{Gaps}}},
  author = {Happe, Jens and Westermann, Dennis and Sachs, Kai and Kapov{\'a}, Lucia},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Heineman, George T. and Kofron, Jan and Plasil, Frantisek},
  year = {2010},
  volume = {6093},
  pages = {20--35},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-13821-8_4},
  abstract = {Software performance engineering (SPE) enables software architects to ensure high performance standards for their applications. However, applying SPE in practice is still challenging. Most enterprise applications include a large software basis, such as middleware and legacy systems. In many cases, the software basis is the determining factor of the system's overall timing behavior, throughput, and resource utilization. To capture these influences on the overall system's performance, established performance prediction methods (modelbased and analytical) rely on models that describe the performance-relevant aspects of the system under study. Creating such models requires detailed knowledge on the system's structure and behavior that, in most cases, is not available. In this paper, we abstract from the internal structure of the system under study. We focus on message-oriented middleware (MOM) and analyze the dependency between the MOM's usage and its performance. We use statistical inference to conclude these dependencies from observations. For ActiveMQ 5.3, the resulting functions predict the performance with a relative mean square error 0.1.},
  file = {/home/stefan/Zotero/storage/ZN59Z63R/Happe et al. - 2010 - Statistical Inference of Software Performance Mode.pdf},
  isbn = {978-3-642-13820-1 978-3-642-13821-8},
  language = {en}
}

@inproceedings{jamshidi_learning_2018,
  title = {Learning to Sample: Exploiting Similarities across Environments to Learn Performance Models for Configurable Systems},
  shorttitle = {Learning to Sample},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}} - {{ESEC}}/{{FSE}} 2018},
  author = {Jamshidi, Pooyan and Velez, Miguel and K{\"a}stner, Christian and Siegmund, Norbert},
  year = {2018},
  pages = {71--82},
  publisher = {{ACM Press}},
  address = {{Lake Buena Vista, FL, USA}},
  doi = {10.1145/3236024.3236074},
  abstract = {Most software systems provide options that allow users to tailor the system in terms of functionality and qualities. The increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non-functional properties. To identify how options and interactions affect the performance of a system, several sampling and learning strategies have been recently proposed. However, existing approaches usually assume a fixed environment (hardware, workload, software release) such that learning has to be repeated once the environment changes. Repeating learning and measurement for each environment is expensive and often practically infeasible. Instead, we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transferlearning strategies. Based on empirical insights about common relationships regarding (i) influential options, (ii) their interactions, and (iii) their performance distributions, our approach, L2S (Learning to Sample), selects better samples in the target environment based on information from the source environment. It progressively shrinks and adaptively concentrates on interesting regions of the configuration space. With both synthetic benchmarks and several real systems, we demonstrate that L2S outperforms state of the art performance learning and transfer-learning approaches in terms of measurement effort and learning accuracy.},
  file = {/home/stefan/Zotero/storage/ZPIQF7TQ/Jamshidi et al. - 2018 - Learning to sample exploiting similarities across.pdf},
  isbn = {978-1-4503-5573-5},
  language = {en}
}

@article{jamshidi_transfer_2017,
  title = {Transfer {{Learning}} for {{Performance Modeling}} of {{Configurable Systems}}: {{An Exploratory Analysis}}},
  shorttitle = {Transfer {{Learning}} for {{Performance Modeling}} of {{Configurable Systems}}},
  author = {Jamshidi, Pooyan and Siegmund, Norbert and Velez, Miguel and K{\"a}stner, Christian and Patel, Akshay and Agarwal, Yuvraj},
  year = {2017},
  month = sep,
  abstract = {Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.},
  archivePrefix = {arXiv},
  eprint = {1709.02280},
  eprinttype = {arxiv},
  file = {/home/stefan/Zotero/storage/84XD7V44/Jamshidi et al. - 2017 - Transfer Learning for Performance Modeling of Conf.pdf},
  journal = {arXiv:1709.02280 [cs, stat]},
  keywords = {Computer Science - Performance,Computer Science - Software Engineering,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jin_understanding_2012,
  title = {Understanding and Detecting Real-World Performance Bugs},
  author = {Jin, Guoliang and Song, Linhai and Shi, Xiaoming and Scherpelz, Joel and Lu, Shan},
  year = {2012},
  month = jun,
  volume = {47},
  pages = {77--88},
  issn = {0362-1340},
  doi = {10.1145/2345156.2254075},
  abstract = {Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs. This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs. Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.},
  file = {/home/stefan/Zotero/storage/Q8CUSCYU/Jin et al. - 2012 - Understanding and detecting real-world performance.pdf},
  journal = {ACM SIGPLAN Notices},
  keywords = {characteristics study,performance bugs,rule-based bug detection},
  number = {6}
}

@article{kalbarczyk_analyzing_nodate,
  title = {Analyzing {{Performance Differences}} between {{Multiple Code Versions}}},
  author = {Kalbarczyk, Tomasz and Imam, Noah and Zhang, Tianyi and Gotimukul, Yamini and Boyles, Brian and Patel, Sushen},
  pages = {9},
  abstract = {Optimizing code performance has always been an important goal of programmers who code at a low-level, and is gaining importance even in high-level programs due to the proliferation of mobile and distributed applications with limited processing and memory resources. Nevertheless, software developers are often ignorant of the way their code modifications affect performance until it is too late or quite expensive to rectify the problem. Diff utilities are a wellestablished method for reviewing code changes and identifying bugs on enterprise scale software projects, but they only provide information on syntactic changes. We propose a tool, named PerfDiff, that enables developers to quickly identify performance changes at function-level granularity between two versions of a program. Our approach uses an existing profiling tool to obtain performance data on two versions of a program, and then compares the performance of changed functions. The performance measure tracked by PerfDiff is the execution time of the functions.},
  file = {/home/stefan/Zotero/storage/SAKBGBSK/Kalbarczyk et al. - Analyzing Performance Differences between Multiple.pdf},
  language = {en}
}

@inproceedings{kaltenecker_distance-based_2019,
  title = {Distance-{{Based Sampling}} of {{Software Configuration Spaces}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
  year = {2019},
  month = may,
  pages = {1084--1094},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICSE.2019.00112},
  abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
  file = {/home/stefan/Zotero/storage/3AXEC4QN/Kaltenecker et al. - 2019 - Distance-Based Sampling of Software Configuration .pdf},
  isbn = {978-1-72810-869-8},
  language = {en}
}

@article{kolesnikov_relation_2019,
  title = {On the Relation of Control-Flow and Performance Feature Interactions: A Case Study},
  shorttitle = {On the Relation of Control-Flow and Performance Feature Interactions},
  author = {Kolesnikov, Sergiy and Siegmund, Norbert and K{\"a}stner, Christian and Apel, Sven},
  year = {2019},
  month = aug,
  volume = {24},
  pages = {2410--2437},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-019-09705-w},
  abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
  file = {/home/stefan/Zotero/storage/WV5UFR7S/Kolesnikov et al. - 2019 - On the relation of control-flow and performance fe.pdf},
  journal = {Empirical Software Engineering},
  language = {en},
  number = {4}
}

@article{kolesnikov_tradeoffs_2019,
  title = {Tradeoffs in Modeling Performance of Highly Configurable Software Systems},
  author = {Kolesnikov, Sergiy and Siegmund, Norbert and K{\"a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
  year = {2019},
  month = jun,
  volume = {18},
  pages = {2265--2283},
  issn = {1619-1366, 1619-1374},
  doi = {10.1007/s10270-018-0662-9},
  abstract = {Modeling the performance of a highly-configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment.},
  file = {/home/stefan/Zotero/storage/S3HMIDZP/Kolesnikov et al. - 2019 - Tradeoffs in modeling performance of highly config.pdf},
  journal = {Software \& Systems Modeling},
  language = {en},
  number = {3}
}

@article{lesenich_indicators_2018,
  title = {Indicators for Merge Conflicts in the Wild: Survey and Empirical Study},
  shorttitle = {Indicators for Merge Conflicts in the Wild},
  author = {Le{\ss}enich, Olaf and Siegmund, Janet and Apel, Sven and K{\"a}stner, Christian and Hunsen, Claus},
  year = {2018},
  month = jun,
  volume = {25},
  pages = {279--313},
  issn = {0928-8910, 1573-7535},
  doi = {10.1007/s10515-017-0227-0},
  abstract = {While the creation of new branches and forks is easy and fast with modern version-control systems, merging is often time-consuming. Especially when dealing with many branches or forks, a prediction of merge costs based on lightweight indicators would be desirable to help developers recognize problematic merging scenarios before potential conflicts become too severe in the evolution of a complex software project. We analyze the predictive power of several indicators, such as the number, size or scattering degree of commits in each branch, derived either from the versioncontrol system or directly from the source code. Based on a survey of 41 developers, we inferred 7 potential indicators to predict the number of merge conflicts. We tested corresponding hypotheses by studying 163 open-source projects, including 21,488 merge scenarios and comprising 49,449,773 lines of code. A notable (negative) result is that none of the 7 indicators suggested by the participants of the developer survey has a predictive power concerning the frequency of merge conflicts. We discuss this and other findings as well as perspectives thereof.},
  file = {/home/stefan/Zotero/storage/RR2RF4NZ/Leßenich et al. - 2018 - Indicators for merge conflicts in the wild survey.pdf},
  journal = {Automated Software Engineering},
  language = {en},
  number = {2}
}

@inproceedings{li_pilot_2016,
  title = {Pilot: {{A Framework}} That {{Understands How}} to {{Do Performance Benchmarks}} the {{Right Way}}},
  shorttitle = {Pilot},
  booktitle = {2016 {{IEEE}} 24th {{International Symposium}} on {{Modeling}}, {{Analysis}} and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Li, Yan and Gupta, Yash and Miller, Ethan L. and Long, Darrell D. E.},
  year = {2016},
  month = sep,
  pages = {169--178},
  issn = {2375-0227},
  doi = {10.1109/MASCOTS.2016.31},
  abstract = {Carrying out even the simplest performance benchmark requires considerable knowledge of statistics and computer systems, and painstakingly following many error-prone steps, which are distinct skill sets yet essential for getting statistically valid results. As a result, many performance measurements in peer-reviewed publications are flawed. Among many problems, they fall short in one or more of the following requirements: accuracy, precision, comparability, repeatability, and control of overhead. This is a serious problem because poor performance measurements misguide system design and optimization. We propose a collection of algorithms and heuristics to automate these steps. They cover the collection, storing, analysis, and comparison of performance measurements. We implement these methods as a readily-usable open source software framework called Pilot, which can help to reduce human error and shorten benchmark time. Evaluation of Pilot on various benchmarks show that it can reduce the cost and complexity of running benchmarks, and can produce better measurement results.},
  file = {/home/stefan/Zotero/storage/F6GBH5YH/7774578.html},
  keywords = {benchmark testing,Benchmark testing,benchmark time,computer performance,computer systems,Computers,error-prone steps,heuristic algorithms,human error reduction,measurement techniques,Measurement uncertainty,peer-reviewed publications,performance analysis,performance benchmarks,performance evaluation,Performance evaluation,performance measurements,Pilot,public domain software,readily-usable open source software framework,software performance,software performance evaluation,system design,system optimization,system performance,Throughput,Tuning}
}

@article{lung_experience_nodate,
  title = {{{EXPERIENCE OF COMMUNICATIONS SOFTWARE EVOLUTION AND PERFORMANCE IMPROVEMENT WITH PATTERNS}}},
  author = {Lung, Chung-Horng and Zhao, Qiang and Xu, Hui and Mar, Heine and Kanagaratnam, Prem},
  pages = {6},
  abstract = {Software evolves as requirements or technologies change. Tremendous efforts are often needed to support software evolution as evolution may involve reverse engineering and subsequent restructuring or forward engineering. Design patterns have captured great attentions as they provide rapid transfer of proven solutions. The paper presents an experimental study of applying design patterns to restructuring in communications software. The restructured software not only satisfies the new functional requirements, but also increases the performance. The paper demonstrates the benefit by showing concrete performance results to support the improvement.},
  file = {/home/stefan/Zotero/storage/VX9T6SQ9/Lung et al. - EXPERIENCE OF COMMUNICATIONS SOFTWARE EVOLUTION AN.pdf},
  language = {en}
}

@article{mathew_finding_2019,
  title = {Finding {{Trends}} in {{Software Research}}},
  author = {Mathew, George and Agrawal, Amritanshu and Menzies, Tim},
  year = {2019},
  pages = {1--1},
  issn = {0098-5589, 1939-3520, 2326-3881},
  doi = {10.1109/TSE.2018.2870388},
  abstract = {This paper explores the structure of research papers in software engineering. Using text mining, we study 35,391 software engineering (SE) papers from 34 leading SE venues over the last 25 years. These venues were divided, nearly evenly, between conferences and journals. An important aspect of this analysis is that it is fully automated and repeatable. To achieve that automation, we used a stable topic modeling technique called LDADE that fully automates parameter tuning in LDA. Using LDADE, we mine 11 topics that represent much of the structure of contemporary SE. The 11 topics presented here should not be "set in stone" as the only topics worthy of study in SE. Rather our goal is to report that (a) text mining methods can detect large scale trends within our community; (b) those topic change with time; so (c) it is important to have automatic agents that can update our understanding of our community whenever new data arrives.},
  archivePrefix = {arXiv},
  eprint = {1608.08100},
  eprinttype = {arxiv},
  file = {/home/stefan/Zotero/storage/X288SGQM/Mathew et al. - 2019 - Finding Trends in Software Research.pdf;/home/stefan/Zotero/storage/LW7T5J6S/1608.html},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {Computer Science - Software Engineering}
}

@inproceedings{melo_how_2016,
  title = {How Does the Degree of Variability Affect Bug Finding?},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Melo, Jean and Brabrand, Claus and W{\k{a}}sowski, Andrzej},
  year = {2016},
  month = may,
  pages = {679--690},
  publisher = {{Association for Computing Machinery}},
  address = {{Austin, Texas}},
  doi = {10.1145/2884781.2884831},
  abstract = {Software projects embrace variability to increase adaptability and to lower cost; however, others blame variability for increasing complexity and making reasoning about programs more difficult. We carry out a controlled experiment to quantify the impact of variability on debugging of preprocessor-based programs. We measure speed and precision for bug finding tasks defined at three different degrees of variability on several subject programs derived from real systems. The results show that the speed of bug finding decreases linearly with the degree of variability, while effectiveness of finding bugs is relatively independent of the degree of variability. Still, identifying the set of configurations in which the bug manifests itself is difficult already for a low degree of variability. Surprisingly, identifying the exact set of affected configurations appears to be harder than finding the bug in the first place. The difficulty in reasoning about several configurations is a likely reason why the variability bugs are actually introduced in configurable programs. We hope that the detailed findings presented here will inspire the creation of programmer support tools addressing the challenges faced by developers when reasoning about configurations, contributing to more effective debugging and, ultimately, fewer bugs in highly-configurable systems.},
  file = {/home/stefan/Zotero/storage/7TUYJJUH/Melo et al. - 2016 - How does the degree of variability affect bug find.pdf},
  isbn = {978-1-4503-3900-1},
  keywords = {bug finding,preprocessors,variability},
  series = {{{ICSE}} '16}
}

@inproceedings{mordahl_empirical_2019,
  title = {An Empirical Study of Real-World Variability Bugs Detected by Variability-Oblivious Tools},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Mordahl, Austin and Oh, Jeho and Koc, Ugur and Wei, Shiyi and Gazzillo, Paul},
  year = {2019},
  month = aug,
  pages = {50--61},
  publisher = {{Association for Computing Machinery}},
  address = {{Tallinn, Estonia}},
  doi = {10.1145/3338906.3338967},
  abstract = {Many critical software systems developed in C utilize compile-time configurability. The many possible configurations of this software make bug detection through static analysis difficult. While variability-aware static analyses have been developed, there remains a gap between those and state-of-the-art static bug detection tools. In order to collect data on how such tools may perform and to develop real-world benchmarks, we present a way to leverage configuration sampling, off-the-shelf ``variability-oblivious'' bug detectors, and automatic feature identification techniques to simulate a variability-aware analysis. We instantiate our approach using four popular static analysis tools on three highly configurable, real-world C projects, obtaining 36,061 warnings, 80\% of which are variability warnings. We analyze the warnings we collect from these experiments, finding that most results are variability warnings of a variety of kinds such as NULL dereference. We then manually investigate these warnings to produce a benchmark of 77 confirmed true bugs (52 of which are variability bugs) useful for future development of variability-aware analyses.},
  file = {/home/stefan/Zotero/storage/3FWJS9PD/Mordahl et al. - 2019 - An empirical study of real-world variability bugs .pdf},
  isbn = {978-1-4503-5572-8},
  keywords = {configurable software,static analysis,variability bugs},
  series = {{{ESEC}}/{{FSE}} 2019}
}

@inproceedings{mostafa_tracking_2009,
  title = {Tracking Performance across Software Revisions},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Principles}} and {{Practice}} of {{Programming}} in {{Java}} - {{PPPJ}} '09},
  author = {Mostafa, Nagy and Krintz, Chandra},
  year = {2009},
  pages = {162},
  publisher = {{ACM Press}},
  address = {{Calgary, Alberta, Canada}},
  doi = {10.1145/1596655.1596682},
  abstract = {Repository-based revision control systems such as CVS, RCS, Subversion, and GIT, are extremely useful tools that enable software developers to concurrently modify source code, manage conflicting changes, and commit updates as new revisions. Such systems facilitate collaboration with and concurrent contribution to shared source code by large developer bases. In this work, we investigate a framework for ``performance-aware'' repository and revision control for Java programs. Our system automatically tracks behavioral differences across revisions to provide developers with feedback as to how their change impacts performance of the application. It does so as part of the repository commit process by profiling the performance of the program or component, and performing automatic analyses that identify differences in the dynamic behavior or performance between two code revisions.},
  file = {/home/stefan/Zotero/storage/M9B8LFVT/Mostafa und Krintz - 2009 - Tracking performance across software revisions.pdf},
  isbn = {978-1-60558-598-7},
  language = {en}
}

@inproceedings{muhlbauer_accurate_2019,
  title = {Accurate {{Modeling}} of {{Performance Histories}} for {{Evolving Software Systems}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {M{\"u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
  year = {2019},
  month = nov,
  pages = {640--652},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/ASE.2019.00065},
  abstract = {Learning from the history of a software system's performance behavior does not only help discovering and locating performance bugs, but also identifying evolutionary performance patterns and general trends, such as when technical debt accumulates. Exhaustive regression testing is usually impractical, because rigorous performance benchmarking requires executing a realistic workload per revision, which results in large execution times. In this paper, we propose a novel active revision sampling approach, which aims at tracking and understanding a system's performance history by approximating the performance behavior of a software system across all of its revisions. In a nutshell, we iteratively sample and measure the performance of specific revisions that help us building an exact performance-evolution model, and we use Gaussian Process models to assess in which revision ranges our model is most uncertain with the goal to sample further revisions for measurement. We have conducted an empirical analysis of the evolutionary performance behavior modeled as a time series of the histories of six real-world software systems. Our evaluation demonstrates that Gaussian Process models are able to accurately estimate the performanceevolution history of real-world software systems with only few measurements and to reveal interesting behaviors and trends.},
  copyright = {All rights reserved},
  file = {/home/stefan/Zotero/storage/DBNHY4B7/Muhlbauer et al. - 2019 - Accurate Modeling of Performance Histories for Evo.pdf},
  isbn = {978-1-72812-508-4},
  language = {en}
}

@article{mytkowicz_evaluating_2010,
  title = {Evaluating the Accuracy of {{Java}} Profilers},
  author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  year = {2010},
  month = jun,
  volume = {45},
  pages = {187--197},
  issn = {0362-1340},
  doi = {10.1145/1809028.1806618},
  abstract = {Performance analysts profile their programs to find methods that are worth optimizing: the "hot" methods. This paper shows that four commonly-used Java profilers (xprof , hprof , jprofile, and yourkit) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement. This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly. We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.},
  file = {/home/stefan/Zotero/storage/G3VX5CD2/Mytkowicz et al. - 2010 - Evaluating the accuracy of Java profilers.pdf},
  journal = {ACM SIGPLAN Notices},
  keywords = {bias,observer effect,profiling},
  number = {6}
}

@article{mytkowicz_producing_2009,
  title = {Producing Wrong Data without Doing Anything Obviously Wrong!},
  author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  year = {2009},
  month = mar,
  volume = {44},
  pages = {265--276},
  issn = {0362-1340},
  doi = {10.1145/1508284.1508275},
  abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
  file = {/home/stefan/Zotero/storage/8W6MDINC/Mytkowicz et al. - 2009 - Producing wrong data without doing anything obviou.pdf},
  journal = {ACM SIGPLAN Notices},
  keywords = {bias,measurement,performance},
  number = {3}
}

@article{nair_finding_2018,
  title = {Finding {{Faster Configurations}} Using {{FLASH}}},
  author = {Nair, Vivek and Yu, Zhe and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
  year = {2018},
  month = jan,
  volume = {PP},
  doi = {10.1109/TSE.2018.2870895},
  abstract = {Finding good configurations for a software system is often challenging since the number of configuration options can be large. Software engineers often make poor choices about configuration or, even worse, they usually use a sub-optimal configuration in production, which leads to inadequate performance. To assist engineers in finding the (near) optimal configuration, this paper introduces FLASH, a sequential model-based method, which sequentially explores the configuration space by reflecting on the configurations evaluated so far to determine the next best configuration to explore. FLASH scales up to software systems that defeat the prior state of the art model-based methods in this area. FLASH runs much faster than existing methods and can solve both single-objective and multi-objective optimization problems. The central insight of this paper is to use the prior knowledge (gained from prior runs) to choose the next promising configuration. This strategy reduces the effort (i.e., number of measurements) required to find the (near) optimal configuration. We evaluate FLASH using 30 scenarios based on 7 software systems to demonstrate that FLASH saves effort in 100\% and 80\% of cases in single-objective and multi-objective problems respectively by up to several orders of magnitude compared to the state of the art techniques.},
  file = {/home/stefan/Zotero/storage/LXB9QM3Q/Nair et al. - 2018 - Finding Faster Configurations using FLASH.pdf},
  journal = {IEEE Transactions on Software Engineering}
}

@article{nair_using_2017,
  title = {Using {{Bad Learners}} to Find {{Good Configurations}}},
  author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
  year = {2017},
  pages = {257--267},
  doi = {10.1145/3106237.3106238},
  abstract = {Finding the optimally performing con guration of a so ware system for a given se ing is o en challenging. Recent approaches address this challenge by learning performance models based on a sample set of con gurations. However, building an accurate performance model can be very expensive (and is o en infeasible in practice). e central insight of this paper is that exact performance values (e.g., the response time of a so ware system) are not required to rank con gurations and to identify the optimal one. As shown by our experiments, performance models that are cheap to learn but inaccurate (with respect to the di erence between actual and predicted performance) can still be used rank con gurations and hence nd the optimal con guration. is novel rank-based approach allows us to signi cantly reduce the cost (in terms of number of measurements of sample con guration) as well as the time required to build performance models. We evaluate our approach with 21 scenarios based on 9 so ware systems and demonstrate that our approach is bene cial in 16 scenarios; for the remaining 5 scenarios, an accurate model can be built by using very few samples anyway, without the need for a rank-based approach.},
  archivePrefix = {arXiv},
  eprint = {1702.05701},
  eprinttype = {arxiv},
  file = {/home/stefan/Zotero/storage/6TH4QUUZ/Nair et al. - 2017 - Using Bad Learners to find Good Configurations.pdf},
  journal = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering - ESEC/FSE 2017},
  keywords = {Computer Science - Software Engineering},
  language = {en}
}

@inproceedings{nguyen_automated_2012,
  title = {Automated Detection of Performance Regressions Using Statistical Process Control Techniques},
  booktitle = {Proceedings of the 3rd {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Nguyen, Thanh H.D. and Adams, Bram and Jiang, Zhen Ming and Hassan, Ahmed E. and Nasser, Mohamed and Flora, Parminder},
  year = {2012},
  month = apr,
  pages = {299--310},
  publisher = {{Association for Computing Machinery}},
  address = {{Boston, Massachusetts, USA}},
  doi = {10.1145/2188286.2188344},
  abstract = {The goal of performance regression testing is to check for performance regressions in a new version of a software system. Performance regression testing is an important phase in the software development process. Performance regression testing is very time consuming yet there is usually little time assigned for it. A typical test run would output thousands of performance counters. Testers usually have to manually inspect these counters to identify performance regressions. In this paper, we propose an approach to analyze performance counters across test runs using a statistical process control technique called control charts. We evaluate our approach using historical data of a large software team as well as an open-source software project. The results show that our approach can accurately identify performance regressions in both software systems. Feedback from practitioners is very promising due to the simplicity and ease of explanation of the results.},
  file = {/home/stefan/Zotero/storage/UL6BICWS/Nguyen et al. - 2012 - Automated detection of performance regressions usi.pdf},
  isbn = {978-1-4503-1202-8},
  keywords = {load testing,performance engineering,statistical control technique},
  series = {{{ICPE}} '12}
}

@inproceedings{nguyen_using_2012,
  title = {Using Control Charts for Detecting and Understanding Performance Regressions in Large Software},
  booktitle = {In {{Proceedings}} of the 2012 {{IEEE Fifth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}},
  author = {Nguyen, Thanh H. D. and Profile, See and Nguyen, Thanh H. D.},
  year = {2012},
  publisher = {{IEEE Computer Society}},
  abstract = {All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.},
  file = {/home/stefan/Zotero/storage/NBI8BJYI/Nguyen et al. - 2012 - Using control charts for detecting and understandi.pdf;/home/stefan/Zotero/storage/W8QUCRAZ/summary.html}
}

@misc{noauthor_ensuring_nodate,
  title = {Ensuring Stable Performance for Systems That Degrade | {{Proceedings}} of the 5th International Workshop on {{Software}} and Performance},
  file = {/home/stefan/Zotero/storage/S4MF2TXP/1071021.html},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/1071021.1071026?casa\_token=L5ajbbmwMUcAAAAA:sAHqBBq57LzQfzSkttUCcfNOZeTUQW1Py4Ij8G1gcPNQbfSixHlBez\_Z-OYEiJCcXFxSii-hWoE}
}

@inproceedings{oh_finding_2017,
  title = {Finding Near-Optimal Configurations in Product Lines by Random Sampling},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
  year = {2017},
  month = aug,
  pages = {61--71},
  publisher = {{Association for Computing Machinery}},
  address = {{Paderborn, Germany}},
  doi = {10.1145/3106237.3106273},
  abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
  file = {/home/stefan/Zotero/storage/KBA38B2R/Oh et al. - 2017 - Finding near-optimal configurations in product lin.pdf},
  isbn = {978-1-4503-5105-8},
  keywords = {finding optimal configurations,searching configuration spaces,software product lines},
  series = {{{ESEC}}/{{FSE}} 2017}
}

@inproceedings{oliveira_perphecy_2017,
  title = {Perphecy: {{Performance Regression Test Selection Made Simple}} but {{Effective}}},
  shorttitle = {Perphecy},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Oliveira, Augusto Born De and Fischmeister, Sebastian and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  year = {2017},
  month = mar,
  pages = {103--113},
  publisher = {{IEEE}},
  address = {{Tokyo, Japan}},
  doi = {10.1109/ICST.2017.17},
  abstract = {Developers of performance sensitive production software are in a dilemma: performance regression tests are too costly to run at each commit, but skipping the tests delays and complicates performance regression detection. Ideally, developers would have a system that predicts whether a given commit is likely to impact performance and suggests which tests to run to detect a potential performance regression. Prior approaches towards this problem require static or dynamic analyses that limit their generality and applicability. This paper presents an approach that is simple and general, and that works surprisingly well for real applications.},
  file = {/home/stefan/Zotero/storage/HH2D37WH/Oliveira et al. - 2017 - Perphecy Performance Regression Test Selection Ma.pdf},
  isbn = {978-1-5090-6031-3},
  language = {en}
}

@article{ousterhout_always_2018,
  title = {Always Measure One Level Deeper},
  author = {Ousterhout, John},
  year = {2018},
  month = jun,
  volume = {61},
  pages = {74--83},
  issn = {00010782},
  doi = {10.1145/3213770},
  file = {/home/stefan/Zotero/storage/KQX2ED5Z/Ousterhout - 2018 - Always measure one level deeper.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {7}
}

@inproceedings{pinto_automating_2015,
  title = {Automating the {{Assessment}} of the {{Performance Quality Attribute}} for {{Evolving Software Systems}}: {{An Exploratory Study}}},
  shorttitle = {Automating the {{Assessment}} of the {{Performance Quality Attribute}} for {{Evolving Software Systems}}},
  booktitle = {2015 48th {{Hawaii International Conference}} on {{System Sciences}}},
  author = {Pinto, Felipe and Kulesza, Uira and Silva, Leo and Guerra, Eduardo},
  year = {2015},
  month = jan,
  pages = {5144--5153},
  publisher = {{IEEE}},
  address = {{HI, USA}},
  doi = {10.1109/HICSS.2015.608},
  abstract = {This paper describes an exploratory study for the evaluation of the performance quality attribute for releases of the same system. The main aim is to reveal performance degradations of architectural scenarios and their possible causes. Three software systems from different domains are used in our study, including a large-scale web system (SIGAA), a UML modeling tool (ArgoUML), and a client-server framework for development of network applications (Netty). The data collection of the study is accomplished using a scenario-based approach that uses dynamic analysis and code repository mining to provide an automated way to reveal degradations of scenarios on releases of software systems. The results of our study show the feasibility of the approach to determine the causes of the performance degradations of scenarios, including the degraded and changed methods of scenarios, and the issues that have affected them.},
  file = {/home/stefan/Zotero/storage/XBI3QT7V/Pinto et al. - 2015 - Automating the Assessment of the Performance Quali.pdf},
  isbn = {978-1-4799-7367-5},
  language = {en}
}

@inproceedings{rahman_gang_2019,
  title = {Gang of {{Eight}}: {{A Defect Taxonomy}} for {{Infrastructure}} as {{Code Scripts}}},
  shorttitle = {Gang of {{Eight}}},
  author = {Rahman, Akond and Farhana, Effat and Parnin, Chris},
  year = {2019},
  abstract = {Defects in infrastructure as code (IaC) scripts can have serious consequences, for example, creating large-scale system outages. A taxonomy of IaC defects can be useful for understanding the nature of defects, and identifying activities needed to fix and prevent defects in IaC scripts. The goal of this paper is to help practitioners improve the quality of infrastructure as code (IaC) scripts by developing a defect taxonomy for IaC scripts through qualitative analysis. We develop a taxonomy of IaC defects by applying qualitative analysis on 1,448 defect-related commits collected from open source software (OSS) repositories of the Openstack organization. We conduct a survey with 66 practitioners to assess if they agree with the identified defect categories included in our taxonomy. We quantify the frequency of identified defect categories by analyzing 80,425 commits collected from 291 OSS repositories spanning across 2005 to 2019. Our defect taxonomy for IaC consists of eight categories, including a category specific to IaC called idempotency (i.e., defects that lead to incorrect system provisioning when the same IaC script is executed multiple times). We observe the surveyed 66 practitioners to agree most with idempotency. The most frequent defect category is configuration data i.e., providing erroneous configuration data in IaC scripts. Our taxonomy and the quantified frequency of the defect categories can help practitioners to improve IaC script quality by prioritizing verification and validation efforts.}
}

@inproceedings{reichelt_how_2018,
  title = {How to {{Detect Performance Changes}} in {{Software History}}: {{Performance Analysis}} of {{Software System Versions}}},
  shorttitle = {How to {{Detect Performance Changes}} in {{Software History}}},
  booktitle = {Companion of the 2018 {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Reichelt, David Georg and K{\"u}hne, Stefan},
  year = {2018},
  month = apr,
  pages = {183--188},
  publisher = {{Association for Computing Machinery}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3185768.3186404},
  abstract = {Source code changes can affect the performance of software. Structured knowledge about classes of those changes could guide software developers in avoiding negative changes and improving the performance by positive changes. Neither a comprehensive overview nor a mature method for structured detection of those changes exists for this purpose. We address this research challenge by presenting Performance Analysis of Software Systems (PeASS). PeASS builds up a comprehensive knowledge base of changes affecting the performance of a software by analyzing the version history of a repository using its unit tests. It is based on a method for determining the significant performance changes between two unit tests by measurement and statistical analysis. Furthermore, PeASS uses regression test selection for saving measurement time and root cause isolation method for performance changes analysis. We demonstrate our methodology in the context of Java by analyzing the versions of Apache Commons IO.},
  file = {/home/stefan/Zotero/storage/S4A7P6PG/Reichelt und Kühne - 2018 - How to Detect Performance Changes in Software Hist.pdf},
  isbn = {978-1-4503-5629-9},
  keywords = {mining software repositories,performance testing},
  series = {{{ICPE}} '18}
}

@book{saltelli_global_2008,
  title = {Global Sensitivity Analysis: The Primer},
  shorttitle = {Global Sensitivity Analysis},
  editor = {Saltelli, A.},
  year = {2008},
  publisher = {{John Wiley}},
  address = {{Chichester, England ; Hoboken, NJ}},
  file = {/home/stefan/Zotero/storage/65RPZQ27/Saltelli - 2008 - Global sensitivity analysis the primer.pdf},
  isbn = {978-0-470-05997-5},
  keywords = {Global analysis (Mathematics),Mathematical models,Sensitivity theory (Mathematics)},
  language = {en},
  lccn = {QA402.3 .G557 2008},
  note = {OCLC: ocn180852094}
}

@article{sanchez_variability_2017,
  title = {Variability Testing in the Wild: The {{Drupal}} Case Study},
  shorttitle = {Variability Testing in the Wild},
  author = {S{\'a}nchez, Ana B. and Segura, Sergio and Parejo, Jos{\'e} A. and {Ruiz-Cort{\'e}s}, Antonio},
  year = {2017},
  month = feb,
  volume = {16},
  pages = {173--194},
  issn = {1619-1374},
  doi = {10.1007/s10270-015-0459-z},
  abstract = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
  file = {/home/stefan/Zotero/storage/QQ5G9WSM/Sánchez et al. - 2017 - Variability testing in the wild the Drupal case s.pdf},
  journal = {Software \& Systems Modeling},
  language = {en},
  number = {1}
}

@inproceedings{sandoval_alcocer_learning_2016,
  title = {Learning from {{Source Code History}} to {{Identify Performance Failures}}},
  booktitle = {Proceedings of the 7th {{ACM}}/{{SPEC}} on {{International Conference}} on {{Performance Engineering}} - {{ICPE}} '16},
  author = {Sandoval Alcocer, Juan Pablo and Bergel, Alexandre and Valente, Marco Tulio},
  year = {2016},
  pages = {37--48},
  publisher = {{ACM Press}},
  address = {{Delft, The Netherlands}},
  doi = {10.1145/2851553.2851571},
  abstract = {Source code changes may inadvertently introduce performance regressions. Benchmarking each software version is traditionally employed to identify performance regressions. Although effective, this exhaustive approach is hard to carry out in practice. This paper contrasts source code changes against performance variations. By analyzing 1,288 software versions from 17 open source projects, we identified 10 source code changes leading to a performance variation (improvement or regression). We have produced a cost model to infer whether a software commit introduces a performance variation by analyzing the source code and sampling the execution of a few versions. By profiling the execution of only 17\% of the versions, our model is able to identify 83\% of the performance regressions greater than 5\% and 100\% of the regressions greater than 50\%.},
  file = {/home/stefan/Zotero/storage/XG83X22M/Sandoval Alcocer et al. - 2016 - Learning from Source Code History to Identify Perf.pdf},
  isbn = {978-1-4503-4080-9},
  language = {en}
}

@inproceedings{sandoval_alcocer_performance_2013,
  title = {Performance Evolution Blueprint: {{Understanding}} the Impact of Software Evolution on Performance},
  shorttitle = {Performance Evolution Blueprint},
  booktitle = {2013 {{First IEEE Working Conference}} on {{Software Visualization}} ({{VISSOFT}})},
  author = {Sandoval Alcocer, Juan Pablo and Bergel, Alexandre and Ducasse, Stephane and Denker, Marcus},
  year = {2013},
  month = sep,
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{Eindhoven, Netherlands}},
  doi = {10.1109/VISSOFT.2013.6650523},
  abstract = {Understanding the root of a performance drop or improvement requires analyzing different program executions at a fine grain level. Such an analysis involves dedicated profiling and representation techniques. JProfiler and YourKit, two recognized code profilers fail, on both providing adequate metrics and visual representations, conveying a false sense of the performance variation root.},
  file = {/home/stefan/Zotero/storage/22HVRQN4/Sandoval Alcocer et al. - 2013 - Performance evolution blueprint Understanding the.pdf},
  isbn = {978-1-4799-1457-9},
  language = {en}
}

@inproceedings{sandoval_alcocer_performance_2019,
  title = {Performance {{Evolution Matrix}}: {{Visualizing Performance Variations Along Software Versions}}},
  shorttitle = {Performance {{Evolution Matrix}}},
  booktitle = {2019 {{Working Conference}} on {{Software Visualization}} ({{VISSOFT}})},
  author = {Sandoval Alcocer, Juan Pablo and Beck, Fabian and Bergel, Alexandre},
  year = {2019},
  month = sep,
  pages = {1--11},
  publisher = {{IEEE}},
  address = {{Cleveland, OH, USA}},
  doi = {10.1109/VISSOFT.2019.00009},
  abstract = {Software performance may be significantly affected by source code modifications. Understanding the effect of these changes along different software versions is a challenging and necessary activity to debug performance failures. It is not sufficiently supported by existing profiling tools and visualization approaches. Practitioners would need to manually compare calling context trees and call graphs. We aim at better supporting the comparison of benchmark executions along multiple software versions. We propose Performance Evolution Matrix, an interactive visualization technique that contrasts runtime metrics to source code changes. It combines a comparison of time series data and execution graphs in a matrix layout, showing performance and source code metrics at different levels of granularity. The approach guides practitioners from the high-level identification of a performance regression to the changes that might have caused the issue. We conducted a controlled experiment with 12 participants to provide empirical evidence of the viability of our method. The results indicate that our approach can reduce the effort for identifying sources of performance regressions compared to traditional profiling visualizations.},
  file = {/home/stefan/Zotero/storage/SHEGTCR7/Sandoval Alcocer et al. - 2019 - Performance Evolution Matrix Visualizing Performa.pdf},
  isbn = {978-1-72814-939-4},
  language = {en}
}

@inproceedings{santos_applying_2019,
  title = {Applying and {{Adapting}} a {{Performance Deviation Analysis Tool}} in {{Web Information Systems}}},
  booktitle = {Proceedings of the {{XVIII Brazilian Symposium}} on {{Software Quality}}},
  author = {Santos, Jadson and Medeiros, Marcos and Kulesza, Uir{\'a} and Pinto, Felipe and Moura, Roniceli and Neto, Jos{\'e} Gameleira and Freitas, Guilherme and Silva, Leo},
  year = {2019},
  month = oct,
  pages = {246--255},
  publisher = {{Association for Computing Machinery}},
  address = {{Fortaleza, Brazil}},
  doi = {10.1145/3364641.3364671},
  abstract = {PerfMiner is a tool that allows the identification of performance deviations during the evolution of software systems. This paper describes the experience of applying and adaptating the PerfMiner tool to the performance analysis of large-scale web information systems. The main result of this experience is the development of the Visum tool that allows detecting, analyzing and comparing performance deviations that occur in testing and production environments of a system. We also report performance issues of 4 different categories found in the analyzed systems, as well as specific refactorings that can help to address them. Finally, we also present discussions and lessons learned related to the automation of performance evaluation activities using our tools.},
  file = {/home/stefan/Zotero/storage/4GILXG6U/Santos et al. - 2019 - Applying and Adapting a Performance Deviation Anal.pdf},
  isbn = {978-1-4503-7282-4},
  keywords = {Análise de Desempenho,Mineração de Logs,PerfMiner,Refatoração de Código},
  series = {{{SBQS}}'19}
}

@article{sarkar_cost-efcient_nodate,
  title = {Cost-{{Efficient Sampling}} for {{Performance Prediction}} of {{Configurable Systems}}},
  author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
  pages = {11},
  abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six realworld systems and provide guidelines for stakeholders to predict performance by sampling.},
  file = {/home/stefan/Zotero/storage/2DLHSB8T/Sarkar et al. - Cost-Efﬁcient Sampling for Performance Prediction .pdf},
  language = {en}
}

@techreport{settles_active_2009,
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  year = {2009},
  pages = {46},
  institution = {{University of Wisconsin\textendash{}Madison}},
  abstract = {The key idea behind active learning is that a machine learning algorithm can
achieve greater accuracy with fewer labeled training instances if it is allowed to
choose the data from which is learns. An active learner may ask queries in the
form of unlabeled instances to be labeled by an oracle (e.g., a human annotator).
Active learning is well-motivated in many modern machine learning problems,
where unlabeled data may be abundant but labels are difficult, time-consuming,
or expensive to obtain.
This report provides a general introduction to active learning and a survey of
the literature. This includes a discussion of the scenarios in which queries can
be formulated, and an overview of the query strategy frameworks proposed in
the literature to date. An analysis of the empirical and theoretical evidence for
active learning, a summary of several problem setting variants, and a discussion
of related topics in machine learning research are also presented.},
  file = {/home/stefan/Zotero/storage/96QTDMC5/Settles - Active Learning Literature Survey.pdf},
  language = {en},
  number = {1648},
  type = {Technical {{Report}}}
}

@inproceedings{siegmund_attributed_2017,
  title = {Attributed Variability Models: Outside the Comfort Zone},
  shorttitle = {Attributed Variability Models},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}  - {{ESEC}}/{{FSE}} 2017},
  author = {Siegmund, Norbert and Sobernig, Stefan and Apel, Sven},
  year = {2017},
  pages = {268--278},
  publisher = {{ACM Press}},
  address = {{Paderborn, Germany}},
  doi = {10.1145/3106237.3106251},
  abstract = {Variability models are often enriched with attributes, such as performance, that encode the influence of features on the respective attribute. In spite of their importance, there are only few attributed variability models available that have attribute values obtained from empirical, real-world observations and that cover interactions between features. But, what does it mean for research and practice when staying in the comfort zone of developing algorithms and tools in a setting where artificial attribute values are used and where interactions are neglected? This is the central question that we want to answer here. To leave the comfort zone, we use a combination of kernel density estimation and a genetic algorithm to rescale a given (real-world) attribute-value profile to a given variability model. To demonstrate the influence and relevance of realistic attribute values and interactions, we present a replication of a widely recognized, third-party study, into which we introduce realistic attribute values and interactions. We found statistically significant differences between the original study and the replication. We infer lessons learned to conduct experiments that involve attributed variability models. We also provide the accompanying tool Thor for generating attribute values including interactions. Our solution is shown to be agnostic about the given input distribution and to scale to large variability models.},
  file = {/home/stefan/Zotero/storage/JHP7MJCL/Siegmund et al. - 2017 - Attributed variability models outside the comfort.pdf},
  isbn = {978-1-4503-5105-8},
  language = {en}
}

@inproceedings{siegmund_family-based_2013,
  title = {Family-Based Performance Measurement},
  booktitle = {Proceedings of the 12th International Conference on {{Generative}} Programming: Concepts \& Experiences - {{GPCE}} '13},
  author = {Siegmund, Norbert and {von Rhein}, Alexander and Apel, Sven},
  year = {2013},
  pages = {95--104},
  publisher = {{ACM Press}},
  address = {{Indianapolis, Indiana, USA}},
  doi = {10.1145/2517208.2517209},
  abstract = {Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98 \%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing.},
  file = {/home/stefan/Zotero/storage/UQNMBL9G/Siegmund et al. - 2013 - Family-based performance measurement.pdf},
  isbn = {978-1-4503-2373-4},
  language = {en}
}

@inproceedings{siegmund_performance-influence_2015,
  title = {Performance-Influence Models for Highly Configurable Systems},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}} - {{ESEC}}/{{FSE}} 2015},
  author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K{\"a}stner, Christian},
  year = {2015},
  pages = {284--294},
  publisher = {{ACM Press}},
  address = {{Bergamo, Italy}},
  doi = {10.1145/2786805.2786845},
  abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
  file = {/home/stefan/Zotero/storage/39HPPJTU/Siegmund et al. - 2015 - Performance-influence models for highly configurab.pdf},
  isbn = {978-1-4503-3675-8},
  language = {en}
}

@inproceedings{siegmund_predicting_2012,
  title = {Predicting Performance via Automated Feature-Interaction Detection},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K{\"a}stner, Christian and Apel, Sven and Batory, Don and Rosenmuller, Marko and Saake, Gunter},
  year = {2012},
  month = jun,
  pages = {167--177},
  publisher = {{IEEE}},
  address = {{Zurich}},
  doi = {10.1109/ICSE.2012.6227196},
  abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95 \%.},
  file = {/home/stefan/Zotero/storage/UY8XPAZD/Siegmund et al. - 2012 - Predicting performance via automated feature-inter.pdf},
  isbn = {978-1-4673-1066-6 978-1-4673-1067-3},
  language = {en}
}

@inproceedings{siegmund_views_2015,
  title = {Views on {{Internal}} and {{External Validity}} in {{Empirical Software Engineering}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Siegmund, Janet and Siegmund, Norbert and Apel, Sven},
  year = {2015},
  month = may,
  pages = {9--19},
  publisher = {{IEEE}},
  address = {{Florence, Italy}},
  doi = {10.1109/ICSE.2015.24},
  abstract = {Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.},
  file = {/home/stefan/Zotero/storage/DPN424TP/Siegmund et al. - 2015 - Views on Internal and External Validity in Empiric.pdf},
  isbn = {978-1-4799-1934-5},
  language = {en}
}

@article{thum_classification_2014,
  title = {A {{Classification}} and {{Survey}} of {{Analysis Strategies}} for {{Software Product Lines}}},
  author = {Th{\"u}m, Thomas and Apel, Sven and K{\"a}stner, Christian and Schaefer, Ina and Saake, Gunter},
  year = {2014},
  month = jun,
  volume = {47},
  pages = {6:1--6:45},
  issn = {0360-0300},
  doi = {10.1145/2580950},
  abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
  file = {/home/stefan/Zotero/storage/SLEQC2TF/Thüm et al. - 2014 - A Classification and Survey of Analysis Strategies.pdf},
  journal = {ACM Computing Surveys (CSUR)},
  keywords = {model checking,Product-line analysis,program family,software analysis,software product line,static analysis,theorem proving,type checking},
  number = {1}
}

@inproceedings{voinea_cvsscan_2005,
  title = {{{CVSscan}}: Visualization of Code Evolution},
  shorttitle = {{{CVSscan}}},
  booktitle = {Proceedings of the 2005 {{ACM}} Symposium on {{Software}} Visualization  - {{SoftVis}} '05},
  author = {Voinea, Lucian and Telea, Alex and {van Wijk}, Jarke J.},
  year = {2005},
  pages = {47},
  publisher = {{ACM Press}},
  address = {{St. Louis, Missouri}},
  doi = {10.1145/1056018.1056025},
  abstract = {During the life cycle of a software system, the source code is changed many times. We study how developers can be enabled to get insight in these changes, in order to understand the status, history and structure better, as well as for instance the roles played by various contributors. We present CVSscan, an integrated multiview environment for this. Central is a lineoriented display of the changing code, where each version is represented by a column, and where the horizontal direction is used for time, Separate linked displays show various metrics, as well as the source code itself. A large variety of options is provided to visualize a number of different aspects. Informal user studies demonstrate the efficiency of this approach for real world use cases.},
  file = {/home/stefan/Zotero/storage/IM7YSF8Z/Voinea et al. - 2005 - CVSscan visualization of code evolution.pdf},
  isbn = {978-1-59593-073-6},
  language = {en}
}

@book{voinea_software_2007,
  title = {Software Evolution Visualization},
  author = {Voinea, S.L},
  year = {2007},
  publisher = {{Technische Universiteit Eindhoven}},
  abstract = {Software has today a large penetration in all infrastructure levels of the society. This penetration took place rapidly in the last two decades and continues to increase. In the same time, however, the software industry gets confronted with two increasingly serious challenges: complexity and evolution. The size of software applications is growing larger. This leads to a steep increase in complexity. Additionally, the change in requirements and available technologies leads to softwaremodifications. As a result, a huge amount of code needs to be maintained and updated every year (i.e. the legacy systems problem). Software visualization is a very promising solution to the abovementioned challenges of the software industry. It is a specialized branch of information visualization, which visualizes artifacts related to software and its development process. In this thesis we try to use visualization of software evolution to get insight in the development context of software and in its evolution trends. The main question we try to answer with this is: "How to enable users to get insight in the evolution of a software system?" Our final goal is to improve both software understanding and decision making during the maintenance phase of large software projects. We start by positioning the thesis in the context of related research in the field of both software evolution analysis and visualization. Then we perform an analysis of the software evolution domain to formalize the problems specific to this field. To this end, we propose a generic system evolution model and a structure based meta-model for software description. Consequently, we use these models to give a formal definition of software evolution. Next we propose a visualization model for software evolution, based on the previously introduced software evolution model. The visualization model consists of a number of steps with specific guidelines for building visual representations. Then we present three applications that make use of the proposed visualization model to support real life software evolution analysis scenarios. These applications cover the most commonly used software description models in industry: file as a set of code lines, project as a set of files, and project as a unitary entity. For each application, we formulate relevant use cases, present specific implementation aspects, and discuss results of use case evaluation experiments. We also propose in this thesis a novel visualization of data exchange processes in Peer-to-Peer networks. While this does not address software evolution, it tackles comparable issues, e.g., the dynamic evolution of a set of interrelated data artifacts. The aim of presenting this visualization is twofold. First, we illustrate how to visualize different types of software-related data than purely software source code. Secondly, we show that the visual and interaction techniques that we have developed in the context of software evolution visualization can be put to a good use for other applications as well. We conclude the thesis with an inventory of reoccurring problems and solutions we have discovered in the visualization of software evolution. Additionally, we identify generic issues that transcend the border of the software evolution domain and we present them together with a set of recommendation for their broader applicability. Finally, we outline the remaining open issues, and the possible research directions that can be followed to address them.},
  file = {/home/stefan/Zotero/storage/EWQXIWSB/Voinea - 2007 - Software evolution visualization.pdf},
  isbn = {978-90-386-1099-3},
  language = {en},
  note = {OCLC: 8086789113}
}

@article{wegner_technique_1960,
  title = {A Technique for Counting Ones in a Binary Computer},
  author = {Wegner, Peter},
  year = {1960},
  month = may,
  volume = {3},
  pages = {322},
  issn = {0001-0782},
  doi = {10.1145/367236.367286},
  abstract = {Standard methods of counting binary ones on a computer with a 704 type instruction code require an inner loop which is carried out once for each bit in the machine word. Program 1 (written in SAP language for purposes of illustration) is an example of such a standard program.},
  file = {/home/stefan/Zotero/storage/Y2FA5WEY/Wegner - 1960 - A technique for counting ones in a binary computer.pdf},
  journal = {Communications of the ACM},
  number = {5}
}

@inproceedings{weimer_automatically_2009,
  title = {Automatically Finding Patches Using Genetic Programming},
  booktitle = {2009 {{IEEE}} 31st {{International Conference}} on {{Software Engineering}}},
  author = {Weimer, Westley and Nguyen, ThanhVu and Le Goues, Claire and Forrest, Stephanie},
  year = {2009},
  pages = {364--374},
  publisher = {{IEEE}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1109/ICSE.2009.5070536},
  abstract = {Automatic program repair has been a longstanding goal in software engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a program fault is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report experimental results demonstrating that it can successfully repair ten different C programs totaling 63,000 lines in under 200 seconds, on average.},
  file = {/home/stefan/Zotero/storage/VIP5DDF4/Weimer et al. - 2009 - Automatically finding patches using genetic progra.pdf},
  isbn = {978-1-4244-3453-4},
  language = {en}
}

@inproceedings{wen_exploring_2019,
  title = {Exploring and Exploiting the Correlations between Bug-Inducing and Bug-Fixing Commits},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Wen, Ming and Wu, Rongxin and Liu, Yepang and Tian, Yongqiang and Xie, Xuan and Cheung, Shing-Chi and Su, Zhendong},
  year = {2019},
  month = aug,
  pages = {326--337},
  publisher = {{Association for Computing Machinery}},
  address = {{Tallinn, Estonia}},
  doi = {10.1145/3338906.3338962},
  abstract = {Bug-inducing commits provide important information to understand when and how bugs were introduced. Therefore, they have been extensively investigated by existing studies and frequently leveraged to facilitate bug fixings in industrial practices. Due to the importance of bug-inducing commits in software debugging, we are motivated to conduct the first systematic empirical study to explore the correlations between bug-inducing and bug-fixing commits in terms of code elements and modifications. To facilitate the study, we collected the inducing and fixing commits for 333 bugs from seven large open-source projects. The empirical findings reveal important and significant correlations between a bug's inducing and fixing commits. We further exploit the usefulness of such correlation findings from two aspects. First, they explain why the SZZ algorithm, the most widely-adopted approach to collecting bug-inducing commits, is imprecise. In view of SZZ's imprecision, we revisited the findings of previous studies based on SZZ, and found that 8 out of 10 previous findings are significantly affected by SZZ's imprecision. Second, they shed lights on the design of automated debugging techniques. For demonstration, we designed approaches that exploit the correlations with respect to statements and change actions. Our experiments on Defects4J show that our approaches can boost the performance of fault localization significantly and also advance existing APR techniques.},
  file = {/home/stefan/Zotero/storage/6NTSBVRE/Wen et al. - 2019 - Exploring and exploiting the correlations between .pdf},
  isbn = {978-1-4503-5572-8},
  keywords = {bug-inducing commits,Empirical study,fault localization and repair},
  series = {{{ESEC}}/{{FSE}} 2019}
}

@inproceedings{zaman_qualitative_2012,
  title = {A Qualitative Study on Performance Bugs},
  booktitle = {2012 9th {{IEEE Working Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Zaman, S. and Adams, B. and Hassan, A. E.},
  year = {2012},
  month = jun,
  pages = {199--208},
  publisher = {{IEEE}},
  address = {{Zurich}},
  doi = {10.1109/MSR.2012.6224281},
  abstract = {Software performance is one of the important qualities that makes software stand out in a competitive market. However, in earlier work we found that performance bugs take more time to fix, need to be fixed by more experienced developers and require changes to more code than non-performance bugs. In order to be able to improve the resolution of performance bugs, a better understanding is needed of the current practice and shortcomings of reporting, reproducing, tracking and fixing performance bugs. This paper qualitatively studies a random sample of 400 performance and non-performance bug reports of Mozilla Firefox and Google Chrome across four dimensions (Impact, Context, Fix and Fix validation). We found that developers and users face problems in reproducing performance bugs and have to spend more time discussing performance bugs than other kinds of bugs. Sometimes performance regressions are tolerated as a tradeoff to improve something else.},
  file = {/home/stefan/Zotero/storage/FQXS4A29/Zaman et al. - 2012 - A qualitative study on performance bugs.pdf},
  isbn = {978-1-4673-1761-0 978-1-4673-1760-3},
  language = {en}
}

@inproceedings{zhang_ensembles_2005,
  title = {Ensembles of {{Models}} for {{Automated Diagnosis}} of {{System Performance Problems}}},
  booktitle = {2005 {{International Conference}} on {{Dependable Systems}} and {{Networks}} ({{DSN}}'05)},
  author = {Zhang, S. and Cohen, I. and Goldszmidt, M. and Symons, J. and Fox, A.},
  year = {2005},
  pages = {644--653},
  publisher = {{IEEE}},
  address = {{Yokohama, Japan}},
  doi = {10.1109/DSN.2005.44},
  abstract = {Violations of service level objectives (SLO) in Internet services are urgent conditions requiring immediate attention. Previously we explored [1] an approach for identifying which lowlevel system properties were correlated to high-level SLO violations (the metric attribution problem). The approach is based on automatically inducing models from data using pattern recognition and probability modeling techniques. In this paper we extend our approach to adapt to changing workloads and external disturbances by maintaining an ensemble of probabilistic models, adding new models when existing ones do not accurately capture current system behavior. Using realistic workloads on an implemented prototype system, we show that the ensemble of models captures the performance behavior of the system accurately under changing workloads and conditions. We fuse information from the models in the ensemble to identify likely causes of the performance problem, with results comparable to those produced by an oracle that continuously changes the model based on advance knowledge of the workload. The cost of inducing new models and managing the ensembles is negligible, making our approach both immediately practical and theoretically appealing.},
  file = {/home/stefan/Zotero/storage/K9SZS96I/Zhang et al. - 2005 - Ensembles of Models for Automated Diagnosis of Sys.pdf},
  isbn = {978-0-7695-2282-1},
  language = {en}
}

@inproceedings{henard_combining_2015,
	author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
	title = {Combining Multi-Objective Search and Constraint Solving for Configuring Large Software Product Lines},
	year = {2015},
	isbn = {9781479919345},
	publisher = {IEEE Press},
	booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
	pages = {517–528},
	numpages = {12},
	location = {Florence, Italy},
	series = {ICSE ’15}
}

@inproceedings{medeiros_comparison_2016,
	author = {Medeiros, Fl\'{a}vio and K\"{a}stner, Christian and Ribeiro, M\'{a}rcio and Gheyi, Rohit and Apel, Sven},
	title = {A Comparison of 10 Sampling Algorithms for Configurable Systems},
	year = {2016},
	isbn = {9781450339001},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2884781.2884793},
	doi = {10.1145/2884781.2884793},
	booktitle = {Proceedings of the 38th International Conference on Software Engineering},
	pages = {643–654},
	numpages = {12},
	location = {Austin, Texas},
	series = {ICSE ’16}
}

@inproceedings{alves_sampling_2020,
	author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
	title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
	year = {2020},
	isbn = {9781450369916},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3358960.3379137},
	doi = {10.1145/3358960.3379137},
	booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
	pages = {277–288},
	numpages = {12},
	keywords = {machine learning, performance prediction, configurable systems, software product lines},
	location = {Edmonton AB, Canada},
	series = {ICPE ’20}
}

@inproceedings{rabkin_static_2011,
	author = {Rabkin, Ariel and Katz, Randy},
	title = {Static Extraction of Program Configuration Options},
	year = {2011},
	isbn = {9781450304450},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1985793.1985812},
	doi = {10.1145/1985793.1985812},
	booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
	pages = {131–140},
	numpages = {10},
	keywords = {configuration, documentation, experiences, static analysis},
	location = {Waikiki, Honolulu, HI, USA},
	series = {ICSE ’11}
}

@inproceedings{10.1109/ICSM.2013.59,
	author = {Hill, Emily and Bacchelli, Alberto and Binkley, Dave and Dit, Bogdan and Lawrie, Dawn and Oliveto, Rocco},
	title = {Which Feature Location Technique is Better?},
	year = {2013},
	isbn = {9780769549811},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ICSM.2013.59},
	doi = {10.1109/ICSM.2013.59},
	booktitle = {Proceedings of the 2013 IEEE International Conference on Software Maintenance},
	pages = {408–411},
	numpages = {4},
	keywords = {Relevance measures, Concern location, Feature location, Empirical studies},
	series = {ICSM ’13}
}



